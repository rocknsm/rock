filter {
  if [@metadata][stage] == "broraw_kafka" {
    # Tags will determine if there is some sort of parse failure
    if ![tags] {
      # Set the timestamp
      date { match => [ "ts", "ISO8601" ] }

      # move metadata to new field
      mutate {
        rename => {
          "@stream" => "[@meta][stream]"
          "@system" => "[@meta][system]"
          "@proc"   => "[@meta][proc]"
        }
      }

      # Rename ID field from file analyzer logs
      if [@meta][stream] in ["pe", "x509", "files"] {
      mutate { rename => { "id" => "fuid" } }
      mutate {
      add_field => { "[@meta][event_type]" => "file" }
      add_field => { "[@meta][id]" => "%{fuid}" }
      }
      } else if [@meta][stream] in ["intel", "notice", "notice_alarm", "signatures", "traceroute"] {
        mutate { add_field => { "[@meta][event_type]" => "detection" } }
      } else if [@meta][stream] in [ "capture_loss", "cluster", "communication", "loaded_scripts", "packet_filter", "prof", "reporter", "stats", "stderr", "stdout" ] {
        mutate { add_field => { "[@meta][event_type]" => "diagnostic" } }
      } else if [@meta][stream] in ["netcontrol", "netcontrol_drop", "netcontrol_shunt", "netcontrol_catch_release", "openflow"] {
        mutate { add_field => { "[@meta][event_type]" => "netcontrol" } }
      } else if [@meta][stream] in ["known_certs", "known_devices", "known_hosts", "known_modbus", "known_services", "software"] {
        mutate { add_field => { "[@meta][event_type]" => "observations" } }
      } else if [@meta][stream] in ["barnyard2", "dpd", "unified2", "weird"] {
        mutate { add_field => { "[@meta][event_type]" => "miscellaneous" } }
      } else {

        # Network type
        mutate {
          convert => {
          "id_orig_p" => "integer"
          "id_resp_p" => "integer"
          }
          add_field => {
            "[@meta][event_type]" => "network"
            "[@meta][id]" => "%{uid}"
            "[@meta][orig_host]" => "%{id_orig_h}"
            "[@meta][orig_port]" => "%{id_orig_p}"
            "[@meta][resp_host]" => "%{id_resp_h}"
            "[@meta][resp_port]" => "%{id_resp_p}"
          }
        }
        geoip {
          source => "id_orig_h"
          target => "[@meta][geoip_orig]"
        }
        geoip {
          source => "id_resp_h"
          target => "[@meta][geoip_resp]"
        }
      }

      # Tie related records
      mutate { add_field => { "[@meta][related_ids]" => [] }}
      if [uid] {
        mutate { merge => {"[@meta][related_ids]" => "uid" }}
      }
      if [fuid] {
        mutate { merge => {"[@meta][related_ids]" => "fuid" }}
      }
      if [related_fuids] {
        mutate { merge => { "[@meta][related_ids]" => "related_fuids" }}
      }
      if [orig_fuids] {
        mutate { merge => { "[@meta][related_ids]" => "orig_fuids" }}
      }
      if [resp_fuids] {
        mutate { merge => { "[@meta][related_ids]" => "resp_fuids" }}
      }
      if [conn_uids] {
        mutate { merge => { "[@meta][related_ids]" => "conn_uids" }}
      }
      if [cert_chain_fuids] {
        mutate { merge => { "[@meta][related_ids]" => "cert_chain_fuids" }}
      }

      # Nest the entire document
      ruby {
        code => "
          require 'logstash/event'

          logtype = event.get('[@meta][stream]')
          ev_hash = event.to_hash
          meta_hash = ev_hash['@meta']
          timestamp = ev_hash['@timestamp']

          # Cleanup duplicate info
          #meta_hash.delete('stream')
          ev_hash.delete('@meta')
          ev_hash.delete('@timestamp')
          ev_hash.delete('tags')

          result = {
          logtype => ev_hash,
          '@meta' => meta_hash,
          '@timestamp' => timestamp
          }
          event.initialize( result )
        "
      }
      mutate { add_field => {"[@metadata][stage]" => "broraw_kafka" } }
    }
    else {
      mutate { add_field => { "[@metadata][stage]" => "_parsefailure" } }
    }
  }
}
