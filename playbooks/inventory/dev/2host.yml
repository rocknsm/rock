---
# The inventory defines which hosts belong to which groups and what variables are applied to them. The playbooks
# themselves in our scheme do not define any variables. The names you see on the far left here correspond to the limit
# function you can run with the ansible-playbook command. For example ansible-playbook site.yml --limit sensor would
# look at this inventory file and see the hosts that are part of the sensor group. When site.yml imports the sensors.yml
# file, sensor.yml will only apply itself to the servers listed in the sensors group in this inventory file.

all:
  vars:
    bpf_filter: "" # Use this to screen traffic from the Moloch PCAP capture. Format is <IP_ADDRESS>/<CIDR>
    dns_ip: # <192.168.1.50> The IP address of the sensor's DNS server. You must define this.
    es_datas: 0 # How many elasticsearch data nodes you would like to run
    es_masters: 5 # How many elasticsearch masters you would like to run
    es_mem: 8 # The amount of memory you want to assign to each elasticsearch instance
    elastic_pv_size: 2700 # The amount of space allocated in GB to each persistent volume for Elasticsearch
    home_net: # This is used to define the homenet for bro/suricata
      - "192.168.0.0/16"
      - "10.0.0.0/8"
      - "172.16.0.0/12"
    kafka_mem: 1 # Kafka JVM Memory
    kafka_pv_size: 10 # The amount of space allocated in GB to each persistent volume for Kafka
    pcap_bpf_filter: "" # Packets that match this filter will NOT be saved to disk by Moloch
    pcap_cluster: false # Set to true to utilize clustered storage for pcap files
    pcap_pv_size: 0 # The amount of space allocated in GB to each persistent volume for Moloch
    # services_cidr is the range of addresses kubernetes will use for external services like Moloch and Kibana
    # It must be at least a /28
    services_cidr: 172.16.30.52-59
    zookeeper_mem: 1 # Zookeeper JVM Memory
    zookeeper_pv_size: 10 # The amount of space allocated in GB to each persistent volume for Zookeeper
    zookeeper_replicas: 3 # Number of zookeeper replicas to be created
    use_ceph_for_pcap: false

    # Advanced Moloch settings, do not touch unless you know what you're doing
    moloch_spiDataMaxIndices: 5
    moloch_pcapWriteMethod: "simple"
    moloch_pcapWriteSize: 262143
    moloch_dbBulkSize: 300000
    moloch_maxESConns: 30
    moloch_maxESRequests: 500
    moloch_packetsPerPoll: 50000
    moloch_magicMode: "libmagic"
    moloch_maxPacketsInQueue: 200000
    pcap_folder: '/pcap'

  children:

    # Any host in this group will be used in the ceph cluster. By default this is all hosts except remote-sensors
    ceph:
      children:
        master-server:
        servers:
        sensors:

    # Any host in this group will be eligible for use in the elasticsearch cluster. By default this is all hosts except remote-sensors
    elasticsearch:
      children:
        master-server:
        servers:

    # Any host in this group will be eligible to run logstash. By default this is any host that also has elasticsearch
    logstash:
      children:
        elasticsearch:

    # Any host in this group will be eligible to run kibana. By default this is any host that also has elasticsearch
    kibana:
      children:
        elasticsearch:

    # Any host in this group will run Kafka. By default this is all sensors and remote sensors
    kafka:
      children:
        sensors:
        remote-sensors:

    # Any host in this group will run Bro. By default this is all sensors and remote sensors
    bro:
      children:
        sensors:
        remote-sensors:

    # Any host in this group will run Moloch. By default this is all sensors and remote sensors
    moloch:
      children:
        sensors:
        remote-sensors:

    # Any host in this group will run Suricata. By default this is all sensors and remote sensors
    suricata:
      children:
        sensors:
        remote-sensors:

    nodes:

      children:

        sensors:
          hosts:
            rocksensor21.lan:
              ansible_user: root
              ansible_connection: ssh
              management_ipv4: 172.16.30.60 # Ex: 192.168.1.198 This is the IP address of the node. You must define this.
              bro_workers: 5
              moloch_threads: 8
              monitor_interfaces: # This is the interface the sensor in question will listen on.
              - eno2 # Ex: ens4
              data_disk_devices: # These are the disks that ceph will use. It must be a full physical disk
              # - /dev/vdb (You must define this)
              pcap_disk: sdb

            rocksensor22.lan:
              ansible_user: root
              ansible_connection: ssh
              management_ipv4: 172.16.30.61
              bro_workers: 5
              moloch_threads: 8
              monitor_interfaces:
              - eno2
              data_disk_devices:
              pcap_disk: sdb

            rocksensor23.lan:
              ansible_user: root
              ansible_connection: ssh
              management_ipv4: 172.16.30.62
              bro_workers: 5
              moloch_threads: 8
              monitor_interfaces:
              - eno2
              data_disk_devices:
              pcap_disk: sdb

            rocksensor24.lan:
              ansible_user: root
              ansible_connection: ssh
              management_ipv4: 172.16.30.63
              bro_workers: 5
              moloch_threads: 8
              monitor_interfaces:
              - eno2
              data_disk_devices:
              pcap_disk: sdb

            rocksensor25.lan:
              ansible_user: root
              ansible_connection: ssh
              management_ipv4: 172.16.30.64
              bro_workers: 5
              moloch_threads: 8
              monitor_interfaces:
              - eno2
              data_disk_devices:
              pcap_disk: sdb
        # If you have a deployment where the majority of the kit is in one location, but some sensors remotely
        # deployed away from the central servers put them in this group. This will remove any clustering dependencies
        # among nodes. For example: Kafka will not try to cluster with other kafka instances nearby. If you have
        # no remote sensors, leave the group defined, but everything after the group name empty. Ex: you could
        # have an inventory with just "remote-sensors:", but no hosts defined.
        remote-sensors:
          #hosts:
          #  rocksensor2.lan:
          #    ansible_user: root
          #    bro_workers: 2
          #    moloch_threads: 8
          #    data_disk_devices:
              #- /dev/sdb <raw disk name>
          #    management_ipv4: # IP Address of node
          #    monitor_interfaces:
              #- eth0 <Interface Name>
        servers:

          hosts:
            rockserver22.lan:
              ansible_user: root
              ansible_connection: ssh
              management_ipv4: 172.16.30.51 #Ex: 192.168.1.194
              data_disk_devices:
              - /dev/sda

        master-server:
          hosts:
            rockserver21.lan:
              ansible_user: root
              ansible_connection: ssh
              management_ipv4: 172.16.30.50 #Ex: 192.168.1.199
              data_disk_devices:
              - /dev/sda

    # This is used when you add -e reinstall=true to your ansible-playbook command. This has the effect
    # of resetting kubernetes on all nodes defined below
    nodes_to_remove:
      # Example: (you need to define your own hosts)
      hosts:
        rocksensor21.lan:
        rocksensor22.lan:
        rocksensor23.lan:
        rocksensor24.lan:
        rocksensor25.lan:
        rockserver21.lan:
        rockserver22.lan:
...
