---
# The inventory defines which hosts belong to which groups and what variables are applied to them. The playbooks
# themselves in our scheme do not define any variables. The names you see on the far left here correspond to the limit
# function you can run with the ansible-playbook command. For example ansible-playbook site.yml --limit sensor would
# look at this inventory file and see the hosts that are part of the sensor group. When site.yml imports the sensors.yml
# file, sensor.yml will only apply itself to the servers listed in the sensors group in this inventory file.

all:
  vars:

    #############################
    # General System Settings   #
    #############################

    # The IP address of the system DNS server. You may define this or it will default
    # to using the master server's management IP. We suggest you leave it to default
    # unless you have a specific reason to use a different DNS server. Keep in mind
    # you will need to manually provide all required DNS entries on your separate
    # DNS Server or the kit will break.
    dns_ip:

    #############################
    # Moloch Settings           #
    #############################

    # Explanations taken from: https://github.com/aol/moloch/wiki/Settings

    # The bpf filter used to reduce traffic. Used both on live and file traffic.
    moloch_bpf: ""

    # Semicolon ';' separated list of bpf filters which when matched for a
    # session causes the remaining pcap from being saved for the session. It is
    # possible to specify the number of packets to save per filter by ending
    # with a :num. For example dontSaveBPFs = port 22:5 will only save 5 packets
    # for port 22 sessions. Currently only the initial packet is matched against
    # the bpfs.
    moloch_dontSaveBPFs: ""

    # There are two storage types available on the system. One is clustered storage
    # and the other is Direct Attached Storage (DAS). Using clustered storage
    # will cause all drives placed in the "ceph_disks" variable list to act as
    # one giant storage device across the entire kit. This is ideal for many types
    # of storage, but causes latency. This is not acceptable for high speed PCAP
    # capture.
    use_ceph_for_pcap: false

    # This should be defined if you are going to use Moloch in direct disk
    # access mode which is the default. If this is defined it is assumed you are
    # going to run Moloch in direct disk access mode instead of using ceph. We recommend
    # you dedicate a separate disk to PCAP. The disk you will use for PCAP is defined
    # on the sensor hosts in pcap_disk. If you define this value, that disk will
    # be formatted and wiped and then /pcap used as a mount point to it. WARNING:
    # THIS WILL BE APPLIED TO ALL SENSORS. YOU CANNOT MIX AND MATCH BETWEEN CLUSTERED
    # AND NON-CLUSTERED STORAGE.
    pcap_folder: '/pcap'

    # If you want to use clustered storage as described above for PCAP, you will
    # need to set this value. This is the amount of space in GBs total that you
    # will assign to PCAP. It should be zero if use_ceph_for_pcap is false.
    # Remember, this will be spread out across every drive in the ceph cluster
    # as defined in the variable "ceph_disks".
    pcap_pv_size: 0

    # Advanced Moloch settings, do not touch unless you know what you're doing
    moloch_spiDataMaxIndices: 5
    moloch_pcapWriteMethod: "simple"
    moloch_pcapWriteSize: 262143
    moloch_dbBulkSize: 300000
    moloch_maxESConns: 30
    moloch_maxESRequests: 500
    moloch_packetsPerPoll: 50000
    moloch_magicMode: "libmagic"
    moloch_maxPacketsInQueue: 200000

    #############################
    # Elasticsearch Settings    #
    #############################

    # The number of Elasticsearch data nodes you would like to run. These are
    # different from elastic_masters in that they do not run the master role
    # Unless your kit will exceed 5 nodes, you should probably leave this at 0
    # unless you know what you are doing.
    elastic_datas: 0

    # The number of Elasticsearch masters you will run
    elastic_masters: 3

    # The amount of memory you want to assign to each Elasticsearch instance
    elastic_mem: 8

    # The amount of space allocated in GB to each persistent volume for Elasticsearch
    elastic_pv_size: 6

    # The maximum number of CPU cores Elasticsearch can utilize. Keep in mind,
    # if Elasticsearch is not under load, it will use fewer, but this allows
    # Elasticsearch to bind these CPUs (which will make them unavailable to other
    # software) which improves performance. These values are currently tuned for
    # an HPDL160. You will need to adjust them for your needs.
    elastic_cpu_maximum: 16

    # This is the minimum  number of cores which must be available for Elasticsearch
    # to start. If other processes have bound cores and fewer than this number
    # are available, than Elasticsearch will not start. Elasticsearch may use
    # less than this, it simply won't start if fewer than this are available. These
    # values are currently tuned for an HPDL160. You will need to adjust them for
    # your needs.
    elastic_cpu_minimum: 12

    #############################
    # Bro/Suricata Settings     #
    #############################

    # This is used to define the homenet for bro/suricata
    home_net:
      - "192.168.0.0/16"
      - "10.0.0.0/8"
      - "172.16.0.0/12"

    #############################
    # Kafka Settings            #
    #############################

    # The amount of memory provided to the Kafka JVM. You probably don't need
    # to change this
    kafka_jvm_memory: 1

    # The amount of space allocated in GB to each persistent volume for Kafka
    # You probably don't need to change this
    kafka_pv_size: 3

    # The amount of memory provided to the Zookeeper JVM. You probably don't need
    # to change this
    zookeeper_mem: 1

    # The amount of space allocated in GB to each persistent volume for Zookeeper
    # You probably don't need to change this
    zookeeper_pv_size: 3

    # Number of zookeeper replicas to be created. Three should be sufficient for
    # any setup. These are only used for replication backup
    zookeeper_replicas: 3

    #############################
    # Kubernetes Settings       #
    #############################

    # services_cidr is the range of addresses kubernetes will use for external services
    # This includes cockpit, Moloch viewer, Kibana, elastichq, kafka-manager, and
    # the kubernetes dashboard. This range must be at least a /28. Ex: "192.168.1.16/28"
    services_cidr: # !!!DEFINE ME!!!

    #############################
    # Software Locations        #
    #############################

    # WARNING!!!: As a rule of thumb, you should not change any of these unless
    # you have a full understanding of the consequences.

  children:

    # Any host in this group will be used in the ceph cluster. By default this is
    # all hosts except remote-sensors
    ceph:
      children:
        master-server:
        servers:
        sensors:

    # Any host in this group will be eligible for use in the elasticsearch cluster.
    # By default this is all hosts except remote-sensors. Keep in mind, just because
    # a node is eligible to run Elasticsearch, doesn't mean it will - only that it
    # can. Kubernetes will decide where to place instances based on resource consumption.
    elasticsearch:
      children:
        master-server:
        servers:

    # Any host in this group will be eligible to run Logstash. By default this is
    # any host that also has Elasticsearch. Keep in mind, just because a node is
    # eligible to run Logstash, doesn't mean it will - only that it can.
    logstash:
      children:
        elasticsearch:

    # Any host in this group will be eligible to run kibana. By default this is
    # any host that also has Elasticsearch. Keep in mind, just because a node is
    # eligible to run Kibana, doesn't mean it will - only that it can. Kubernetes
    # will decide where to place instances based on resource consumption.
    kibana:
      children:
        elasticsearch:

    # Any host in this group will run Kafka. By default this is all sensors and
    # remote sensors
    kafka:
      children:
        sensors:
        remote-sensors:

    # Any host in this group will run Bro. By default this is all sensors and remote
    # sensors
    bro:
      children:
        sensors:
        remote-sensors:

    # Any host in this group will run Moloch. By default this is all sensors and
    # remote sensors
    moloch:
      children:
        sensors:
        remote-sensors:

    # Any host in this group will run Suricata. By default this is all sensors
    # and remote sensors
    suricata:
      children:
        sensors:
        remote-sensors:

    nodes:

      children:

    #############################
    # Sensor Settings           #
    #############################

    # Here you will define any variables specific to each sensor host.

        sensors:
          hosts:

            # This is the hostname of the sensor. Whatever you put here will be
            # used to overwrite the current hostname.
            tfplneumsensor1.lan:

              # This is the user you will use to SSH to each box for setup. This
              # should always be root.
              ansible_user: root

              # This is the Ansible connection type. This should always be SSH.
              ansible_connection: ssh

              # This is the management IP address of the node. Ex: 192.168.1.198
              # This is the address Ansible will use to communicate with.
              management_ipv4: # !!!DEFINE ME!!!

              # This is the number of bro workers which will spawn on the sensor
              bro_workers: 2

              # This is the number of threads which will be dedicated to Moloch
              moloch_threads: 8

              # This is the interface the sensor will use for monitoring on Moloch,
              # bro, and Suricata. Ex: ens4
              monitor_interface: # !!!DEFINE ME!!!

              # Instead of using ceph you can use an entire drive for PCAP storage.
              # This is the drive you will use. Ex: /dev/sdb. This is only used
              # if use_ceph_for_pcap is set to false. pcap_folder must be defined
              # for this to work. pcap_folder will be the mount point.
              pcap_disk: # !!!DEFINE ME!!!

              # These are the disks you will use for Ceph. These must be devices.
              # This cannot be a folder or partition. Ex: /dev/vdb. You may have
              # none on a machine. On the system in total there must be at least
              # two.
              ceph_disks:
              # - <DISK 1>
              # - <DISK 2>

            # ADDITIONAL SENSORS HERE
            # tfplneumsensor1.lan:
            #   ansible_user: root
            #   ansible_connection: ssh
            #   management_ipv4: # !!!DEFINE ME!!!
            #   bro_workers: 2
            #   moloch_threads: 8
            #   monitor_interface: # !!!DEFINE ME!!!
            #   pcap_disk: # !!!DEFINE ME!!!
            #   ceph_disks:
            #   - <DISK 1>
            #   - <DISK 2>

        # If you have a deployment where the majority of the kit is in one location,
        # but some sensors remotely deployed away from the central servers put them
        # in this group. This will remove any clustering dependencies among nodes.
        # For example: Kafka will not try to cluster with other kafka instances
        # nearby. If you have no remote sensors, leave the group defined, but
        #everything after the group name empty. Ex: you could have an inventory
        #with just "remote-sensors:", but no hosts defined.
        remote-sensors:
          #hosts:
          #  tfplneumsensor2.lan:
          #    ansible_user: root
          #    bro_workers: 2
          #    moloch_threads: 8
          #    ceph_disks:
          #    - /dev/sdb <raw disk name>
          #    management_ipv4: # IP Address of node
          #    monitor_interface:
          #    - eth0 <Interface Name>

    #############################
    # Server Settings           #
    #############################

    # See the sensor section for explanation of values. Servers exist primarily
    # to run Elasticsearch and provide the horsepower to run the sensors. You can
    # see the group section for a list of things that run specifically on servers
    # only.

        servers:

          hosts:

            tfplneumserver2.lan:
              ansible_user: root
              ansible_connection: ssh
              management_ipv4: # !!!DEFINE ME!!!

              ceph_disks:
              # - <DISK 1>
              # - <DISK 2>

        master-server:

    #############################
    # Master Server Settings    #
    #############################

    # See the sensor section for explanation of values. There can only be one master
    # server. It's a bit like the Highlander that way. The master server is special
    # in that it runs the Kubernetes master and is responsible for deploying services
    # out to all the other hosts in the cluster. If you're interested in the specifics
    # feel free to dig through the code.

          hosts:
            tfplneumserver1.lan:
              ansible_user: root
              ansible_connection: ssh
              management_ipv4: # !!!DEFINE ME!!!
              ceph_disks:
              # - <DISK 1>
              # - <DISK 2>

    # This is used when you add -e reinstall=true to your ansible-playbook command. This has the effect
    # of resetting kubernetes on all nodes defined below
    nodes_to_remove:
      # Example: (you need to define your own hosts)
      #hosts:
      #  tfplneumsensor1.lan:
      #  tfplneumserver2.lan:
      #  tfplneumserver1.lan:

...
