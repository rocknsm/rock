---
# The inventory defines which hosts belong to which groups and what variables are applied to them. The playbooks
# themselves in our scheme do not define any variables. The names you see on the far left here correspond to the limit
# function you can run with the ansible-playbook command. For example ansible-playbook site.yml --limit sensor would
# look at this inventory file and see the hosts that are part of the sensor group. When site.yml imports the sensors.yml
# file, sensor.yml will only apply itself to the servers listed in the sensors group in this inventory file.

all:
  vars:
    bpf_filter: "" # Use this to screen traffic from the Moloch PCAP capture. Format is <IP_ADDRESS>/<CIDR>
    pcap_bpf_filter: "" # Packets that match this filter will NOT be saved to disk by Moloch
    dns_ip: # <192.168.1.50> The IP address of the system DNS server. You may define this or it will default to using master server. We suggest you leave it to default unless you know what you are doing
    elastic_datas: 0 # How many elasticsearch data nodes you would like to run
    elastic_masters: 3 # How many elasticsearch masters you would like to run
    elastic_mem: 2 # The amount of memory you want to assign to each elasticsearch instance
    elastic_pv_size: 6 # The amount of space allocated in GB to each persistent volume for Elasticsearch
    elastic_cpu_maximum: 16 # This is the maximum number of CPUs elasticsearch may utilize
    elastic_cpu_minimum: 12 # This is the minimum  number of cores which must be available for elasticsearch to start
    home_net: # This is used to define the homenet for bro/suricata
      - "192.168.0.0/16"
      - "10.0.0.0/8"
      - "172.16.0.0/12"
    kafka_mem: 1 # Kafka JVM Memory
    kafka_pv_size: 3 # The amount of space allocated in GB to each persistent volume for Kafka
    use_ceph_for_pcap: false # Set to true to utilize clustered storage for pcap files
    pcap_pv_size: 6 # The amount of space allocated in GB to each persistent volume for Moloch
    # services_cidr is the range of addresses kubernetes will use for external services like Moloch and Kibana
    # It must be at least a /28
    services_cidr: # Ex: "192.168.1.16/28"
    zookeeper_mem: 1 # Zookeeper JVM Memory
    zookeeper_pv_size: 3 # The amount of space allocated in GB to each persistent volume for Zookeeper
    zookeeper_replicas: 3 # Number of zookeeper replicas to be created

    # Advanced Moloch settings, do not touch unless you know what you're doing
    moloch_spiDataMaxIndices: 5
    moloch_pcapWriteMethod: "simple"
    moloch_pcapWriteSize: 262143
    moloch_dbBulkSize: 300000
    moloch_maxESConns: 30
    moloch_maxESRequests: 500
    moloch_packetsPerPoll: 50000
    moloch_magicMode: "libmagic"
    moloch_maxPacketsInQueue: 200000
    pcap_folder: # This should only be defined if you are going to use Moloch in direct disk access mode. If this is defined it is assumed you are going to run Moloch in direct disk access mode instead of using ceph.

  children:

    # Any host in this group will be used in the ceph cluster. By default this is all hosts except remote-sensors
    ceph:
      children:
        master-server:
        servers:
        sensors:

    # Any host in this group will be eligible for use in the elasticsearch cluster. By default this is all hosts except remote-sensors
    elasticsearch:
      children:
        master-server:
        servers:

    # Any host in this group will be eligible to run logstash. By default this is any host that also has elasticsearch
    logstash:
      children:
        elasticsearch:

    # Any host in this group will be eligible to run kibana. By default this is any host that also has elasticsearch
    kibana:
      children:
        elasticsearch:

    # Any host in this group will run Kafka. By default this is all sensors and remote sensors
    kafka:
      children:
        sensors:
        remote-sensors:

    # Any host in this group will run Bro. By default this is all sensors and remote sensors
    bro:
      children:
        sensors:
        remote-sensors:

    # Any host in this group will run Moloch. By default this is all sensors and remote sensors
    moloch:
      children:
        sensors:
        remote-sensors:

    # Any host in this group will run Suricata. By default this is all sensors and remote sensors
    suricata:
      children:
        sensors:
        remote-sensors:

    nodes:

      children:

        sensors:
          hosts:
            rocksensor1.lan:
              ansible_user: root
              ansible_connection: ssh
              management_ipv4: # Ex: 192.168.1.198 This is the IP address of the node. You must define this.
              bro_workers: 2
              moloch_threads: 8
              monitor_interfaces: # This is the interface the sensor in question will listen on.
              - # Ex: ens4
              pcap_disk: # Optionally, instead of using ceph you can use an entire drive for PCAP storage instead of ceph. These are only used if use_ceph_for_pcap is set to false. pcap_folder must be defined for this to work. pcap_folder will be the mount point.
              data_disk_devices: # These are the disks that ceph will use. It must be a full physical disk
              # - /dev/vdb (You must define this)

        # If you have a deployment where the majority of the kit is in one location, but some sensors remotely
        # deployed away from the central servers put them in this group. This will remove any clustering dependencies
        # among nodes. For example: Kafka will not try to cluster with other kafka instances nearby. If you have
        # no remote sensors, leave the group defined, but everything after the group name empty. Ex: you could
        # have an inventory with just "remote-sensors:", but no hosts defined.
        remote-sensors:
          #hosts:
          #  rocksensor2.lan:
          #    ansible_user: root
          #    bro_workers: 2
          #    moloch_threads: 8
          #    data_disk_devices:
              #- /dev/sdb <raw disk name>
          #    management_ipv4: # IP Address of node
          #    monitor_interfaces:
              #- eth0 <Interface Name>
        servers:

          hosts:
            rockserver2.lan:
              ansible_user: root
              ansible_connection: ssh
              management_ipv4: #Ex: 192.168.1.194
              data_disk_devices:
              #- /dev/vdb

        master-server:
          hosts:
            rockserver1.lan:
              ansible_user: root
              ansible_connection: ssh
              management_ipv4: #Ex: 192.168.1.199
              data_disk_devices:
              #- /dev/vdb

    # This is used when you add -e reinstall=true to your ansible-playbook command. This has the effect
    # of resetting kubernetes on all nodes defined below
    nodes_to_remove:
      # Example: (you need to define your own hosts)
      #hosts:
      #  rocksensor1.lan:
      #  rockserver2.lan:
      #  rockserver1.lan:

...
